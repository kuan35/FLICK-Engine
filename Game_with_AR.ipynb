{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize The DNN Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the weights file\n",
    "model_weights =  \"model/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "\n",
    "# Path to the architecture file\n",
    "model_arch = \"model/deploy.prototxt.txt\"\n",
    "\n",
    "# Load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch, model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Face Detection Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold =0.7):\n",
    "    \n",
    "    # Get the height,width of the image\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Apply mean subtraction, and create 4D blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0,(300, 300), (104.0, 117.0, 123.0))\n",
    "    \n",
    "    # Set the new input value for the network\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Run forward pass on the input to compute output\n",
    "    faces = net.forward()\n",
    "    \n",
    "    # Get the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "    \n",
    "    # Get the index of the prediction with highest confidence \n",
    "    i = np.argmax(prediction_scores)\n",
    "    \n",
    "    # Get the face with highest confidence \n",
    "    face = faces[0,0,i]\n",
    "    \n",
    "    # Extract the confidence\n",
    "    confidence = face[2]\n",
    "    \n",
    "    # if confidence value is greater than the threshold\n",
    "    if confidence > threshold:\n",
    "        \n",
    "        # The 4 values at indexes 3 to 6 are the top-left, bottom-right coordinates\n",
    "        # scales to range 0-1.The original coordinates can be found by \n",
    "        # multiplying x,y values with the width,height of the image\n",
    "        box = face[3:7] * np.array([w, h, w, h])\n",
    "        \n",
    "        # The coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therefore needs be quantized to int type\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        \n",
    "        # Draw a bounding box around the face.\n",
    "        annotated_frame = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        output = (annotated_frame, (x1, y1, x2, y2), True, confidence)\n",
    "    \n",
    "    # Return the original frame if no face is detected with high confidence.\n",
    "    else:\n",
    "        output = (image,(),False, 0)\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the **`face_detector()`** function with a real-time camera feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('face Detection', cv2.WINDOW_NORMAL) \n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    annotated_frame, coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('face Detection', annotated_frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Landmarks Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up the facial landmark Detector:\n",
    "\n",
    "Download [detection model from here](https://github.com/italojs/facial-landmarks-recognition/blob/master/shape_predictor_68_face_landmarks.dat) and put it inside the `model` folder inside this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the landmark detector\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the detect_landmarks() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(box, image):\n",
    "    \n",
    "    # For faster results convert the image to gray-scale\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get the coordinates \n",
    "    (x1, y1, x2, y2) = box\n",
    "\n",
    "    # Perform the detection\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x1, y1, x2, y2))\n",
    "    \n",
    "    # Get the numPy array containing the coordinates of the landmarks\n",
    "    landmarks = shape_to_np(shape)\n",
    "    \n",
    "   # Draw the landmark points with circles \n",
    "    for (x, y) in landmarks:\n",
    "        annotated_image = cv2.circle(image, (x, y),2, (0, 127, 255), -1)\n",
    "\n",
    "    return annotated_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the **`shape_to_np()`** helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape): \n",
    "    \n",
    "    # Create an array of shape (68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype=\"int\")\n",
    "    \n",
    "    # Write the x,y coordinates of each landmark into the array\n",
    "    for i in range(0, 68): \n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "        \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test **`detect_landmarks()`** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Landmark Detection', cv2.WINDOW_NORMAL) \n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Landmark Detection',landmark_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Jump Control mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks, ar_threshold = 0.7): \n",
    "    \n",
    "    \n",
    "    # Calculate the euclidean distance labelled as A,B,C\n",
    "    A = hypot(landmarks[50][0] - landmarks[58][0], landmarks[50][1] - landmarks[58][1])\n",
    "    B = hypot(landmarks[52][0] - landmarks[56][0], landmarks[52][1] - landmarks[56][1])\n",
    "    C = hypot(landmarks[48][0] - landmarks[54][0], landmarks[48][1] - landmarks[54][1])\n",
    "    \n",
    "    # Calculate the mouth aspect ratio\n",
    "    # The value of vertical distance A,B is averaged\n",
    "    mouth_aspect_ratio = (A + B) / (2.0 * C)\n",
    "    \n",
    "    # Return True if the value is greater than the threshold\n",
    "    if mouth_aspect_ratio > ar_threshold:\n",
    "        return True, mouth_aspect_ratio\n",
    "    else:\n",
    "        return False, mouth_aspect_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the **`is_mouth_open()`** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Mouth Status', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Adjust the threshold and make sure it's working for you.\n",
    "        mouth_status,_ = is_mouth_open(landmarks, ar_threshold = 0.6)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(frame,'Is Mouth Open: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Mouth Status',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Crouch Control Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_proximity(box,image, proximity_threshold = 325):\n",
    "    \n",
    "    # Get the height and width of the face bounding box\n",
    "    face_width =  box[2]-box[0]\n",
    "    face_height = box[3]-box[1]\n",
    "    \n",
    "    # Draw rectangle to guide the user \n",
    "    # Calculate the angle of diagonal using face width, height \n",
    "    theta = np.arctan(face_height/face_width)\n",
    "     \n",
    "    # Use the angle to calculate height, width of the guide rectangle\n",
    "    guide_height = np.sin(theta)*proximity_threshold\n",
    "    guide_width  = np.cos(theta)*proximity_threshold\n",
    "    \n",
    "    # Calculate the mid-point of the guide rectangle/face bounding box\n",
    "    mid_x,mid_y = (box[2]+box[0])/2 , (box[3]+box[1])/2\n",
    "    \n",
    "    #Calculate to coordinates of top-left and bottom-right corners\n",
    "    guide_topleft = int(mid_x-(guide_width/2)), int(mid_y-(guide_height/2))\n",
    "    guide_bottomright = int(mid_x +(guide_width/2)), int(mid_y + (guide_height/2))\n",
    "    \n",
    "    # Draw the guide rectangle\n",
    "    cv2.rectangle(image, guide_topleft, guide_bottomright, (0, 255, 255), 2)\n",
    "    \n",
    "    # Calculate the diagonal distance of the bounding box\n",
    "    diagonal = hypot(face_width, face_height)\n",
    "    \n",
    "    # Return True if distance greater than the threshold\n",
    "    if diagonal > proximity_threshold:\n",
    "        return True, diagonal\n",
    "    else:\n",
    "        return False, diagonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the **`face_proximity()`** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Face proximity', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Check if face is closer than the defined threshold\n",
    "        is_face_close,_ = face_proximity(box_coords, face_image, proximity_threshold = 325)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(face_image,'Is Face Close: {}'.format(is_face_close),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face proximity',face_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Perform Calibration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Calibration', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if the frame is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Get the current mouth aspect ratio\n",
    "        _,mouth_ar = is_mouth_open(landmarks)\n",
    "    \n",
    "        # Get the current face proximity\n",
    "        _, proximity  = face_proximity(box_coords, face_image)\n",
    "\n",
    "        # Calculate threshold values\n",
    "        ar_threshold = mouth_ar*1.4\n",
    "        proximity_threshold = proximity*1.3\n",
    "\n",
    "        \n",
    "        # Dsiplay the threshold values\n",
    "        cv2.putText(frame, 'Aspect ratio threshold: {:.2f} '.format(ar_threshold), \n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        cv2.putText(frame,'Proximity threshold: {:.2f}'.format(proximity_threshold), \n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "     \n",
    "    # Display the frame\n",
    "    cv2.imshow('Calibration',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Keyboard Automation\n",
    "***Note:*** *When running the following cells in Jupyter Notebook, make sure you don't use the **Shift + Enter** command to run the following code cells. You can use the Run code cell button in the Toolbar.* *Also if the keyboard buttons misbehave then you can also restart the kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will open a context menu\n",
    "pyautogui.click(button='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press space bar. This will scroll down the page in some browsers\n",
    "pyautogui.press('space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To press multiple keys we can pass a list of strings to **`press()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will move the focus to the next cell in the notebook\n",
    "pyautogui.press(['shift','enter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use **`pyautogui.keyDown()`**  instead of **`pyautogui.press()`**. Then the specified key is held down unless  **`pyautogui.keyUp()`**event takes place helping us trigger a longer key press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold down the shift key\n",
    "pyautogui.keyDown('shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press enter while the shift key is down, this will run the next code cell\n",
    "pyautogui.press('enter')\n",
    "\n",
    "\n",
    "\n",
    "# Release the shift key\n",
    "pyautogui.keyUp('shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ran\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This will run automatically after running the two code cells above\n",
    "print('I ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7:  Build The Final Application\n",
    "\n",
    "Go to **`Chrome://Dino`** in your Chrome browser and run the code cell below.\n",
    "\n",
    "***Note:*** *The image window screen will freeze when you trigger key buttons since at that moment the while loop will pause to press the key. So don't worry about that, after the program launches minimize the camera window and just go to chrome://dino/ and start playing using your face and mouth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保持頭部正向，進行校準...\n",
      "校準完成，頭部轉動閾值: 0.2708498152066725\n",
      "開始遊戲控制！使用:\n",
      "- 張嘴跳躍\n",
      "- 靠近攝像頭下蹲\n",
      "- 頭部左轉向左移動\n",
      "- 頭部右轉向右移動\n"
     ]
    }
   ],
   "source": [
    "# 添加頭部左右轉動檢測功能\n",
    "def detect_head_turn(landmarks, threshold=0.05):\n",
    "    \"\"\"\n",
    "    檢測頭部左右轉動\n",
    "    使用鼻尖相對於臉部中心的位置來判斷頭部轉向\n",
    "    \n",
    "    參數:\n",
    "    landmarks - 面部68個關鍵點\n",
    "    threshold - 偏移閾值，超過此值判定為頭部轉向\n",
    "    \n",
    "    返回:\n",
    "    direction - 轉向方向 (\"left\", \"right\", \"center\")\n",
    "    ratio - 偏移比例，用於判斷轉向程度\n",
    "    \"\"\"\n",
    "    # 計算臉部中心位置（兩側臉頰中點）\n",
    "    face_left = landmarks[0][0]  # 左臉頰點\n",
    "    face_right = landmarks[16][0]  # 右臉頰點\n",
    "    face_center_x = (face_left + face_right) / 2\n",
    "    \n",
    "    # 獲取鼻尖位置\n",
    "    nose_tip_x = landmarks[30][0]\n",
    "    \n",
    "    # 計算鼻尖相對於臉部中心的偏移比例\n",
    "    # 標準化為臉部寬度的比例\n",
    "    face_width = abs(face_right - face_left)\n",
    "    shift_ratio = (nose_tip_x - face_center_x) / face_width\n",
    "    \n",
    "    # 判斷頭部轉向\n",
    "    if shift_ratio < -threshold:\n",
    "        return \"left\", shift_ratio\n",
    "    elif shift_ratio > threshold:\n",
    "        return \"right\", shift_ratio\n",
    "    else:\n",
    "        return \"center\", shift_ratio\n",
    "\n",
    "# 在主循環中整合頭部轉向檢測\n",
    "# 替換原本的主循環\n",
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Dino with OpenCV', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# By default each key press is followed by a 0.1 second pause\n",
    "pyautogui.PAUSE = 0.0\n",
    "\n",
    "# 設定頭部轉動的閾值\n",
    "head_turn_threshold = 0.07  # 可以根據實際情況調整\n",
    "\n",
    "# 設定校準階段\n",
    "print(\"保持頭部正向，進行校準...\")\n",
    "calibration_frames = 30\n",
    "head_positions = []\n",
    "\n",
    "# 校準階段收集正常頭部位置數據\n",
    "for _ in range(calibration_frames):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        _, shift_ratio = detect_head_turn(landmarks, 0)\n",
    "        head_positions.append(shift_ratio)\n",
    "        \n",
    "        cv2.putText(frame, f\"校準中: {len(head_positions)}/{calibration_frames}\", \n",
    "                   (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Dino with OpenCV', frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# 計算個人化頭部轉動閾值\n",
    "if head_positions:\n",
    "    avg_position = sum(head_positions) / len(head_positions)\n",
    "    head_turn_threshold = 2.0 * max(0.03, abs(avg_position))  # 最小閾值為0.03\n",
    "    print(f\"校準完成，頭部轉動閾值: {head_turn_threshold}\")\n",
    "\n",
    "print(\"開始遊戲控制！使用:\")\n",
    "print(\"- 張嘴跳躍\")\n",
    "print(\"- 靠近攝像頭下蹲\")\n",
    "print(\"- 頭部左轉向左移動\")\n",
    "print(\"- 頭部右轉向右移動\")\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # 1. 檢測嘴巴狀態控制跳躍 (空格鍵)\n",
    "        is_open, _ = is_mouth_open(landmarks, ar_threshold)\n",
    "        if is_open:\n",
    "            pyautogui.keyDown('up')\n",
    "            mouth_status = 'Open'\n",
    "        else:\n",
    "            pyautogui.keyUp('up')\n",
    "            mouth_status = 'Closed'\n",
    "        \n",
    "        # 2. 檢測臉部靠近程度控制下蹲 (下鍵)\n",
    "        is_closer, _ = face_proximity(box_coords, frame, proximity_threshold)\n",
    "        if is_closer:\n",
    "            pyautogui.keyDown('down')\n",
    "            proximity_status = 'Close'\n",
    "        else:\n",
    "            pyautogui.keyUp('down')\n",
    "            proximity_status = 'Far'\n",
    "        \n",
    "        # 3. 新增：檢測頭部左右轉動控制左右移動 (左右鍵)\n",
    "        head_direction, turn_ratio = detect_head_turn(landmarks, head_turn_threshold)\n",
    "        \n",
    "        # 根據頭部轉向控制鍵盤\n",
    "        if head_direction == \"left\":\n",
    "            # 按下左鍵或A鍵 (可選擇其一)\n",
    "            pyautogui.keyDown('left')  # 或使用 'a'\n",
    "            pyautogui.keyUp('right')   # 確保右鍵被釋放\n",
    "            turn_status = 'Left'\n",
    "        elif head_direction == \"right\":\n",
    "            # 按下右鍵或D鍵 (可選擇其一)\n",
    "            pyautogui.keyDown('right')  # 或使用 'd'\n",
    "            pyautogui.keyUp('left')     # 確保左鍵被釋放\n",
    "            turn_status = 'Right'\n",
    "        else:\n",
    "            # 如果頭部居中，釋放左右鍵\n",
    "            pyautogui.keyUp('left')\n",
    "            pyautogui.keyUp('right')\n",
    "            turn_status = 'Center'\n",
    "        \n",
    "        # 在畫面上顯示所有控制狀態\n",
    "        cv2.putText(frame, f'Mouth: {mouth_status}', \n",
    "                   (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255), 2)\n",
    "        cv2.putText(frame, f'Face: {proximity_status}', \n",
    "                   (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255), 2)\n",
    "        cv2.putText(frame, f'Head: {turn_status} ({turn_ratio:.2f})', \n",
    "                   (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255), 2)\n",
    "        \n",
    "        # 畫出頭部轉向輔助線\n",
    "        # 臉部中心線\n",
    "        face_center_x = int((landmarks[0][0] + landmarks[16][0]) / 2)\n",
    "        cv2.line(frame, (face_center_x, landmarks[27][1]), \n",
    "                (face_center_x, landmarks[8][1]), (0, 255, 0), 1)\n",
    "        \n",
    "        # 鼻子方向線\n",
    "        nose_line_length = 30\n",
    "        nose_vector_x = landmarks[30][0] - landmarks[27][0]\n",
    "        nose_vector_y = landmarks[30][1] - landmarks[27][1]\n",
    "        nose_length = max(1, hypot(nose_vector_x, nose_vector_y))\n",
    "        nose_direction_x = int(landmarks[30][0] + nose_vector_x/nose_length * nose_line_length)\n",
    "        nose_direction_y = int(landmarks[30][1] + nose_vector_y/nose_length * nose_line_length)\n",
    "        cv2.line(frame, (landmarks[30][0], landmarks[30][1]), \n",
    "                (nose_direction_x, nose_direction_y), (0, 0, 255), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Dino with OpenCV', frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trex-game",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
